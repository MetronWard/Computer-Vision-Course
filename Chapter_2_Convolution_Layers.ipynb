{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7p7gIOh/QCHUaN/kby4Kl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MetronWard/Computer-Vision-Course/blob/main/Chapter_2_Convolution_Layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "i4EgU0ZXzBtt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "HKghd_8pys-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9evBQ1CqxIhR",
        "outputId": "ca0efc78-c03d-4e05-effa-62738c2e7acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/guava-disease-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"asadullahgalib/guava-disease-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_folder = \"/root/.cache/kagglehub/datasets/asadullahgalib/guava-disease-dataset/versions/6/GuavaDiseaseDataset/GuavaDiseaseDataset/\""
      ],
      "metadata": {
        "id": "7scBp9Jyz8yW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (224, 224)\n",
        "img_channels = 3\n",
        "batch_size = 64\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "cXY_pL2dy6oC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "Y1vc_Uw7y2ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageFolder(root=ds_folder + \"train/\",\n",
        "                            transform=T.Compose([\n",
        "                                T.Resize(img_size),\n",
        "                                T.RandomHorizontalFlip(0.5),\n",
        "                                T.RandomVerticalFlip(0.5),\n",
        "                                T.ToTensor(),\n",
        "                                T.Normalize([0.5]*3, [0.5]*3)]),\n",
        "                            )"
      ],
      "metadata": {
        "id": "FKS1BtPFz6w8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImageFolder(root=ds_folder + \"test/\",\n",
        "                            transform=T.Compose([\n",
        "                                T.Resize(img_size),\n",
        "                                T.ToTensor(),\n",
        "                                T.Normalize([0.5]*3, [0.5]*3)]),\n",
        "                            )"
      ],
      "metadata": {
        "id": "HbqBHJBl0-sa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = ImageFolder(root=ds_folder + \"val/\",\n",
        "                            transform=T.Compose([\n",
        "                                T.Resize(img_size),\n",
        "                                T.ToTensor(),\n",
        "                                T.Normalize([0.5]*3, [0.5]*3)]),\n",
        "                            )"
      ],
      "metadata": {
        "id": "Aa-blkKk1J_g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "pEjSHbd01N4z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Functins"
      ],
      "metadata": {
        "id": "EXvnokzw12WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, data_loader, criterion, optimizer, scheduler):\n",
        "\n",
        "  n_samples = len(data_loader.dataset)\n",
        "\n",
        "  progress_bar = tqdm(data_loader, desc=\"Training\")\n",
        "  total_loss = 0.0\n",
        "  total_correct = 0.0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for image, label in progress_bar:\n",
        "    images, labels = image.cuda(), label.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(images)\n",
        "    predicted_classes = torch.argmax(predictions, dim=1)\n",
        "    correct_classes = torch.sum(predicted_classes == labels)\n",
        "    loss = criterion(predictions, labels)\n",
        "\n",
        "    total_correct += correct_classes.item()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  scheduler.step()\n",
        "\n",
        "  return total_loss/n_samples, total_correct/n_samples"
      ],
      "metadata": {
        "id": "PLpmcbBK15vq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  n_samples = len(data_loader.dataset)\n",
        "\n",
        "  progress_bar = tqdm(data_loader, desc=\"Evaluating\")\n",
        "  total_loss = 0.0\n",
        "  total_correct = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for image, label in progress_bar:\n",
        "      images, labels = image.cuda(), label.cuda()\n",
        "\n",
        "      predictions = model(images)\n",
        "      predicted_classes = torch.argmax(predictions, dim=1)\n",
        "      correct_classes = torch.sum(predicted_classes == labels)\n",
        "      loss = criterion(predictions, labels)\n",
        "\n",
        "      total_correct += correct_classes.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss/n_samples, total_correct/n_samples"
      ],
      "metadata": {
        "id": "54XJyd_76gW4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epochs, experiment_name):\n",
        "\n",
        "  df = pd.DataFrame(np.empty([epochs, 5]),\n",
        "                    index=np.arange(epochs),\n",
        "                    columns=[\"Train_Loss\", \"Train_Acc\", \"Val_Loss\", \"Val_Acc\", \"lr\"])\n",
        "\n",
        "  highest_acc = 0.00\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    df.iloc[epoch, 0], df.iloc[epoch, 1] = train_step(model=model, data_loader=train_dataloader,\n",
        "                                            criterion=criterion, optimizer=optimizer,\n",
        "                                            scheduler=scheduler)\n",
        "\n",
        "    df.iloc[epoch, 2], df.iloc[epoch, 3] = evaluate(model=model, data_loader=val_dataloader,\n",
        "                                            criterion=criterion)\n",
        "\n",
        "    df.iloc[epoch, 4] = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    print(f\"Training Loss = {df.iloc[epoch, 0]:.3f}, Training Accuracy = {df.iloc[epoch, 1]:.3f}\\nTesting Loss = {df.iloc[epoch, 2]:.3f}, Testing Accuracy = {df.iloc[epoch, 3]:.3f}\")\n",
        "\n",
        "\n",
        "    if df.iloc[epoch, 3] > highest_acc:\n",
        "      torch.save(model.state_dict(), f\"{experiment_name}.pth\")\n",
        "      highest_acc = df.iloc[epoch, 3]\n",
        "      print(f\"New best accuracy: {highest_acc:.3f} at epoch {epoch+1}\")\n",
        "\n",
        "    print(\"*\"*50)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "1j7Q1jSS7jZy"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}